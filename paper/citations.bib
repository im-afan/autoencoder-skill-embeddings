@MISC{coumans2021,
    author =   {Erwin Coumans and Yunfei Bai},
    title =    {PyBullet, a Python module for physics simulation for games, robotics and machine learning},
    howpublished = {\url{http://pybullet.org}},
    year = {2016--2021}
}

@misc{rl-zoo3,
    author = {Raffin, Antonin},
    title = {RL Baselines3 Zoo},
    year = {2020},
    publisher = {GitHub},
    journal = {GitHub repository},
    howpublished = {\url{https://github.com/DLR-RM/rl-baselines3-zoo}},
}

@misc{towers_gymnasium_2023,
    title = {Gymnasium},
    url = {https://zenodo.org/record/8127025},
    abstract = {An API standard for single-agent reinforcement learning environments, with popular reference environments and related utilities (formerly Gym)},
    urldate = {2023-07-08},
    publisher = {Zenodo},
    author = {Towers, Mark and Terry, Jordan K. and Kwiatkowski, Ariel and Balis, John U. and Cola, Gianluca de and Deleu, Tristan and Goulão, Manuel and Kallinteris, Andreas and KG, Arjun and Krimmel, Markus and Perez-Vicente, Rodrigo and Pierré, Andrea and Schulhoff, Sander and Tai, Jun Jet and Shen, Andrew Tan Jin and Younis, Omar G.},
    month = mar,
    year = {2023},
    doi = {10.5281/zenodo.8127026},
}

@article{
    2022-TOG-ASE,
    author = {Peng, Xue Bin and Guo, Yunrong and Halper, Lina and Levine, Sergey and Fidler, Sanja},
    title = {ASE: Large-scale Reusable Adversarial Skill Embeddings for Physically Simulated Characters},
    journal = {ACM Trans. Graph.},
    issue_date = {August 2022},
    volume = {41},
    number = {4},
    month = jul,
    year = {2022},
    articleno = {94},
    publisher = {ACM},
    address = {New York, NY, USA},
    keywords = {motion control, physics-based character animation, reinforcement learning}
}

@article{PAHIC2021103690,
    title = {Robot skill learning in latent space of a deep autoencoder neural network},
    journal = {Robotics and Autonomous Systems},
    volume = {135},
    pages = {103690},
    year = {2021},
    issn = {0921-8890},
    doi = {https://doi.org/10.1016/j.robot.2020.103690},
    url = {https://www.sciencedirect.com/science/article/pii/S0921889020305303},
    author = {Rok Pahič and Zvezdan Lončarević and Andrej Gams and Aleš Ude},
    keywords = {Skill learning, Latent space representations, Deep autoencoder neural networks},
    abstract = {Just like humans, robots can improve their performance by practicing, i.e. by performing the desired behavior many times and updating the underlying skill representation using the newly gathered data. In this paper, we propose to implement robot practicing by applying statistical and reinforcement learning (RL) in a latent space of the selected skill representation. The latent space is computed by a deep autoencoder neural network, with the data to train the network generated in simulation. However, we show that the resulting latent space representation is useful also for learning on a real robot. Our simulation and real-world results demonstrate that by exploiting the latent space of the underlying motor skill representation, a significant reduction of the amount of data needed for effective learning by Gaussian Process Regression (GPR) can be achieved. Similarly, the number of RL epochs can be significantly reduced. Finally, it is evident from our results that an autoencoder-based latent space is more effective for these purposes than a latent space computed by principal component analysis.}
}
